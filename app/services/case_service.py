import threading
import os
from app.models.case import Citation
from flask import current_app, json
from app.extensions import db
from app.models.case import Case, Document
from ultis.ai_summary import generate_master_summary_with_citations, summarize_document_content
from ultis.ocr import ContentExtractionService
from ultis.storage import StorageService
from sqlalchemy.orm import joinedload

# Kh·ªüi t·∫°o m·ªôt l·∫ßn ·ªü c·∫•p module ho·∫∑c trong CaseService
extractor = ContentExtractionService()

class CaseService:
    @staticmethod
    def create_case(title, files):
        try:
            # 1. Kh·ªüi t·∫°o Case
            new_case = Case(title=title, status="PROCESSING")
            db.session.add(new_case)
            db.session.flush()

            for file in files:
                # 2. L∆∞u file v·∫≠t l√Ω d√πng StorageService
                rel_path = StorageService.save_file(new_case.id, file)
                
                # 3. L∆∞u b·∫£n ghi Document
                doc = Document(
                    case_id=new_case.id, 
                    file_name=file.filename, 
                    file_url=rel_path,
                    status="UPLOADED"
                )
                db.session.add(doc)

            db.session.commit()

            # 4. K√≠ch ho·∫°t OCR ch·∫°y ng·∫ßm
            app = current_app._get_current_object()
            threading.Thread(
                target=CaseService._run_background_ocr, 
                args=(app, new_case.id)
            ).start()

            return new_case
        except Exception as e:
            db.session.rollback()
            raise e

    @staticmethod
    def _run_background_ocr(app, case_id):
        with app.app_context():
            case = Case.query.get(case_id)
            if not case: return
            upload_base = app.config['UPLOAD_FOLDER']
            
            for doc in case.documents:
                full_path = os.path.join(upload_base, doc.file_url)
                # Th·ª±c hi·ªán b√≥c t√°ch n·ªôi dung
                content_pages = extractor.extract_content(full_path)
                
                if content_pages:
                    doc.raw_content = content_pages
                    # 2. D√πng OpenAI ƒë·ªÉ t√≥m t·∫Øt t·ª´ Raw Content ƒë√≥
                    summary_text = summarize_document_content(content_pages)
                    if summary_text:
                        doc.summary = summary_text
                    doc.status = "SUCCESS"
                else:
                    doc.status = "FAILED"
                db.session.commit()
            print("üîó Generating Master Summary...")
            CaseService.create_master_summary(case_id)
            case.status = "COMPLETED"
            db.session.commit()

    @staticmethod
    def get_all_cases():
        """L·∫•y danh s√°ch r√∫t g·ªçn c√°c v·ª• √°n"""
        return Case.query.order_by(Case.created_at.desc()).all()

    @staticmethod
    def get_case_by_id(case_id):
        """L·∫•y chi ti·∫øt m·ªôt v·ª• √°n k√®m theo Citations v√† Documents"""
        return Case.query.options(
            joinedload(Case.citations).joinedload(Citation.document),
            joinedload(Case.documents)
        ).filter_by(id=case_id).first()
    
    @staticmethod
    def create_master_summary(case_id):
        case = Case.query.get(case_id)
        if not case: return

        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o t·ª´ c√°c file ƒë√£ x·ª≠ l√Ω xong
        doc_summaries = []
        for doc in case.documents:
            if doc.summary:
                doc_summaries.append({
                    "id": str(doc.id),
                    "name": doc.file_name,
                    "summary": doc.summary
                })

        # 2. G·ªçi OpenAI t·∫°o summary
        ai_result_raw = generate_master_summary_with_citations(doc_summaries)
        if not ai_result_raw: return
        
        ai_data = json.loads(ai_result_raw)
        
        # 3. C·∫≠p nh·∫≠t Master Summary cho Case
        # Thay th·∫ø m√£ [ref: uuid] th√†nh [1], [2] ƒë·ªÉ Frontend hi·ªÉn th·ªã ƒë·∫πp
        final_summary = ai_data['summary']
        doc_id_map = {doc['id']: i+1 for i, doc in enumerate(doc_summaries)}
        
        # X√≥a c√°c citation c≈© (n·∫øu c√≥) tr∆∞·ªõc khi t·∫°o m·ªõi
        Citation.query.filter_by(case_id=case_id).delete()

        # 4. L∆∞u Citations v√†o DB
        for idx, doc_id in enumerate(ai_data.get('citations', [])):
            new_citation = Citation(
                case_id=case_id,
                document_id=doc_id,
                citation_index=idx + 1 # S·ªë th·ª© t·ª± hi·ªÉn th·ªã [1], [2]...
            )
            db.session.add(new_citation)
            
            # (T√πy ch·ªçn) Re-format m√£ tr√≠ch d·∫´n trong text t·ª´ UUID sang [index]
            final_summary = final_summary.replace(f"[ref: {doc_id}]", f"[{idx + 1}]")

        case.master_summary = final_summary
        case.status = "COMPLETED"
        db.session.commit()